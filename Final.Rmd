---
title: "Exercise Two"
author: "Ali Prasla, Shreya Trivedi, Reece Wooten, Deeksha Yennam"
date: "August 18, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Flights at ABIA

We used the data corresponding to all departures from the Austin/Bergstrom Airport in 2008 to create a set 12 of heat maps, one for each month of the year that depict the average departure delays(in hours) by aircraft carrier and day of the week.

These charts can help us to get a sense of which aircraft carriers are most delayed on each day of the week for every month. We can also observe how delay patterns changed over the year.

```{r setup, include=FALSE}
# loading all libraries
library(plyr)
library(dplyr)
library(ggplot2)
library(reshape)
library(gplots)
```

```{r warning=FALSE}
ABIA=read.csv('ABIA.csv')
ABIA_dep=ABIA[ABIA$Origin=='AUS',]
ABIA_dep_df=tbl_df(ABIA_dep)
ABIA_dep$DayOfWeek=as.factor(ABIA_dep$DayOfWeek)
ABIA_dep$DayOfWeek =revalue(ABIA_dep$DayOfWeek ,c("1"="Mon","2"="Tue","3"="Wed","4"="Thurs","5"="Fri","6"="Sat","7"="Sun"))
ABIA_dep$Month=as.factor(ABIA_dep$Month)
ABIA_dep$Month =revalue(ABIA_dep$Month ,c("1"="January","2"="February","3"="March","4"="April","5"="May","6"="June","7"="July",
    "8"="August","9"="September","10"="October","11"="November","12"="December"))
```

```{r warning=FALSE,message=FALSE}
color=colorRampPalette(c('white','red'))
for (month in levels(ABIA_dep$Month)){
                    ABIA_dep_month=ABIA_dep[ABIA_dep$Month==month,]
                    ABIA_dep_df=tbl_df(ABIA_dep_month)
                    carrier=group_by(ABIA_dep_df,UniqueCarrier, DayOfWeek)
                    carrier_delay = summarize(carrier, delay = mean(DepDelay, na.rm = TRUE))
                    carrier_de=cast(carrier_delay, UniqueCarrier~DayOfWeek)
                    heatmap.2(as.matrix(carrier_de),Rowv=NA, Colv=NA,na.rm=TRUE,main=month,col=color,trace='none')
                    cat("\n\n\n\n\n")
                    }
```

White cells represent no flights relevant to that particular cell.



##Author Attribution

###Pre-Processing
First, we obtain all authors in the training set
```{r}
allAuthorsInTrainingSet = list.files("ReutersC50/c50train")
```

Copy _readerPlain_ helper function and read libraries

```{r}
library(tm)
library(magrittr)
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

Next, iterate through all training authors to create corpus
```{r}

library(foreach)
l = foreach( i = 1:length(allAuthorsInTrainingSet),.combine = 'c') %do% {
  author = allAuthorsInTrainingSet[i]
  fpath = paste("ReutersC50/c50train/",author,"/*.txt",sep = "")
  file_list = Sys.glob(fpath)
  current = lapply(file_list,readerPlain)
  mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
  current
}
trainingCorpus = Corpus(VectorSource(l))
```

Perform data cleaning
```{r}
trainingCorpus = tm_map(trainingCorpus, content_transformer(tolower)) # make everything lowercase
trainingCorpus = tm_map(trainingCorpus, content_transformer(removeNumbers)) # remove numbers
trainingCorpus = tm_map(trainingCorpus, content_transformer(removePunctuation)) # remove punctuation
trainingCorpus = tm_map(trainingCorpus, content_transformer(stripWhitespace))
trainingCorpus = tm_map(trainingCorpus,content_transformer(removeWords),stopwords("en"))
```

Perform similar operations with test set
```{r}
allAuthorsInTestSet = list.files("ReutersC50/c50test")
l = foreach( i = 1:length(allAuthorsInTrainingSet),.combine = 'c') %do% {
  author = allAuthorsInTestSet[i]
  fpath = paste("ReutersC50/c50test/",author,"/*.txt",sep = "")
  file_list = Sys.glob(fpath)
  current = lapply(file_list,readerPlain)
  mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
  current
}
testCorpus = Corpus(VectorSource(l))
testCorpus = tm_map(testCorpus,content_transformer(removeWords),stopwords("en"))
#testWithoutEN = tm_map(testCorpus,content_transformer(removeWords),stopwords("en"))
```


Using the Corpus, construct a Document Term Matrix object with Term Frequency - Inverse Document Frequency scores
```{r}
#construct Document Term Matrix and convert to TF-IDF
DTMTrainSimple = DocumentTermMatrix(trainingCorpus,control = list(weighting = function(x) weightTfIdf(x)))

```


Finally, drop all the sparse terms from the Document term matrix.
```{r}
dropTermPercent = .95
DTMTrainSimple = removeSparseTerms(DTMTrainSimple,dropTermPercent)
```
### Principle Component Analysis
Run PCA on Training Data
```{r}
pcaTrainSimple = prcomp(DTMTrainSimple,scale = TRUE)
```

Plot variance for each number of Principle Components:
```{r}
par(bg = "gray")
plot(pcaTrainSimple$sdev^2,type = "l",main = "Projection Error for number of Principle Components",col = "blue",xlab = "Number of Principal Components",ylab = "Projection Error (Variance)")
bestComponents = 150
abline(v = bestComponents ,lw = 2,col = "red")
```


###Finishing up Latent Semantic Analysis

Next, construct your Document Vectors by applying Principal Components 
```{r}
x = as.matrix(DTMTrainSimple)
V = t(t(pcaTrainSimple$rotation)[1:bestComponents,])
x = x %*% V
```

As a final part of training set pre-processing, construct your response vector:
```{r}
response = sort(rep(allAuthorsInTrainingSet,50))
```

Format your test set:
```{r}
testCorpus = tm_map(testCorpus, content_transformer(tolower)) # make everything lowercase
testCorpus = tm_map(testCorpus, content_transformer(removeNumbers)) # remove numbers
testCorpus = tm_map(testCorpus, content_transformer(removePunctuation)) # remove punctuation
testCorpus = tm_map(testCorpus, content_transformer(stripWhitespace))
```


Turn Test Corpus into DTM matrix:
```{r}
DTMTestSet = DocumentTermMatrix(testCorpus,control = list(weighting = function(x) weightTfIdf(x)))
```

Apply the training set's PCA:
```{r}
pcaTest = predict(pcaTrainSimple,newdata = DTMTestSet)
testX = pcaTest[,1:bestComponents]
```

Create test set response (same as training set response because of directory file structure) and create classification rate function
```{r}
testResponse = response

getClassificationRate = function(model,testX,testY){
  pred = predict(model,data.frame(testX))
  sum(pred == testY)/length(pred)
}
```

###First Model: Naive Bayes
```{r}
library(naivebayes)
nb = naive_bayes(x = x, y= as.factor(response))
getClassificationRate(nb,testX,testResponse)
```

###Second Model: Ensemable Decision Trees: Bagging
Decision Tree Bagging
```{r}
library(randomForest)
library(doParallel)
registerDoParallel(cores = 6)
testTrees = c(1,2,5,10,20,40,60,100,120,180,200)
bagClass = foreach( i = 1:length(testTrees),.combine = 'c') %dopar%
{
    bag = randomForest::randomForest(as.factor(response) ~.,data = data.frame(x),mtry =bestComponents,ntree = testTrees[i])
    getClassificationRate(bag,testX,testResponse)  
}

```

Plot the outcome of Bagging
```{r}
par(bg = "gray")
plot(testTrees,randomForestClass,type = "l", col = "red",main = "Bagging in Decision Trees: Classification Rate vs Number of Trees",xlab = "Number of Trees",ylab = "Classification Rate")
print(paste("Best Out-of-Sample Classification Rate: ", max(bagClass)))
```
This is still a very low classification rate. Let's try a Gradient Boosted Tree model


###Third Model: Ensemble Decision Tree: Gradient Boosting
```{r}
library(gbm)
boost = gbm::gbm(as.factor(response)~.,data = data.frame(x),n.trees = 100,shrinkage = .01,interaction.depth = 3)
pred = predict(boost,data.frame(testX),n.trees = 50)
pred = colnames(pred)[apply(data.frame(pred),1,which.max)]
sum(pred == testResponse)/length(testResponse)
```
Boosting works better than Random Forest or Bagging and Naive Bayes. Yet, there is still a relatively low classification rate. This author attribution task was very difficult for these models. While this model does perform well over the baseline of 2%, there is plenty of room for improvement.

Using boosting as our model going forward, let's take a look at this model's predictions per author.
```{r}
library(dplyr)
authorClassification = data.frame(pred) %>% group_by(pred) %>% summarise(count = n()) %>% arrange(desc(count))
```


The boosted model was over predicting authors:
```{r}
authorClassification[1:5,]
```

And under predicting authors:
```{r}
authorClassification[(nrow(authorClassification)-5):nrow(authorClassification),]
```
###Author Attribution Next Steps
Given more time, the next step would be to apply more sophisticated and specialized Natural Language Processing strategies like Support Vector Machines or Latent Dirichlet Allocation. We would expect those techniques to work better, although the tree methods provide a solid baseline for how well author attribution could work on this Corpus.

##Association Rule Mining

```{r setup, include=FALSE}
library(arules)
```

###Preprocess and Initial Run
First, import data set using 'read.transactions'. This function let's you import the data set in the format which 'arules' can use
```{r}
groceries = read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', format = 'basket', sep = ',', rm.duplicates = FALSE)
```

Next, run the apriori algorithm on the data set to generate association rules.
Initially run the algorithm with low values for the 'Support' and 'Confidence' and check the rules generated:
```{r}
groc_rules <- apriori(groceries, parameter=list(support=.01, confidence=.5, maxlen=10))
inspect(subset(groc_rules, subset=lift > 2))
```

The 14 rules generated here are the set of all possible association rules which have a support and confidence greater than the thresholds provided.


###Find Power Association Rules

Create subsets of these association rules by altering the 'support', 'confidence' and 'lift' parameters and observing which association rules are filtered out. As 'Lift' is the increase in probability of the "consequent" item set given the "if" (antecedent) item set, higher the Lift, stronger is the association between the two item sets in the association rule. To filter out only the strong association rules we can subset for those rules which have high Lift

In this example, no rules have a lift greater than 3.5. Set the lift threshold to 3.0.
```{r}
inspect(subset(groc_rules, subset=lift > 3))
```

There is a trade off present with this 'Lift' threshold. We could get rules with a Lift greater than 3 but we would have to reduce the minimum 'Support' thresholds. This would give us rules where the association is stronger but, because 'Support' is low for them, the count of item sets that show up in these rules are too low to be considered significant from a sales perspective.

Similarly, getting high values of 'Lift' when 'Confidence' is low does not help, because this happens only when 'Expected Confidence' is also low. Such item sets and the resultant association rules which have low 'Expected Confidence' face significance problems just like low 'Support' rules.
```{r}
inspect(subset(groc_rules, subset=confidence > 0.58))
```
The discovered relationships are reasonable. Dairy product are associated in Item 1 and Fruits/Vegetables are associated in Items 2 and 3. This relationships makes sense not only because of typical geography of a grocery store (Dairy Products together and Fruits/Vegetables together), but also the types of people who would purchase those groceries.

Most powerful association rule:
```{r}
inspect(subset(groc_rules, subset=support > .011 & confidence > 0.58 & lift>3))
```


