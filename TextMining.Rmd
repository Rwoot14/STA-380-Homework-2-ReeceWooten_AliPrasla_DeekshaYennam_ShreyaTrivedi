---
title: "Text Mining"
author: "Ali Prasla, Shreya Trivedi, Reece Wooten, Deeksha Yennam"
date: "August 15, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we obtain all authors in the training set
```{r}
allAuthorsInTrainingSet = list.files("ReutersC50/c50train")
```

Copy _readerPlain_ helper function and read libraries

```{r}
library(tm)
library(magrittr)
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

Next, iterate through all training authors to create corpus

```{r}

library(foreach)
l = foreach( i = 1:length(allAuthorsInTrainingSet),.combine = 'c') %do% {
  author = allAuthorsInTrainingSet[i]
  fpath = paste("ReutersC50/c50train/",author,"/*.txt",sep = "")
  file_list = Sys.glob(fpath)
  current = lapply(file_list,readerPlain)
  mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
  current
}
trainingCorpus = Corpus(VectorSource(l))
```

Complete data pre-processing.

```{r}
trainingCorpus = tm_map(trainingCorpus, content_transformer(tolower)) # make everything lowercase
trainingCorpus = tm_map(trainingCorpus, content_transformer(removeNumbers)) # remove numbers
trainingCorpus = tm_map(trainingCorpus, content_transformer(removePunctuation)) # remove punctuation
trainingCorpus = tm_map(trainingCorpus, content_transformer(stripWhitespace))
```

Split _trainingCorpus_ into three corpus'. Without SMART stopwords, without EN stopwords, and with stopwords
```{r}
trainingCorpus = tm_map(trainingCorpus,content_transformer(removeWords),stopwords("en"))
```

Perform similar operations with test set
```{r}
allAuthorsInTestSet = list.files("ReutersC50/c50test")
l = foreach( i = 1:length(allAuthorsInTrainingSet),.combine = 'c') %do% {
  author = allAuthorsInTestSet[i]
  fpath = paste("ReutersC50/c50test/",author,"/*.txt",sep = "")
  file_list = Sys.glob(fpath)
  current = lapply(file_list,readerPlain)
  mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
  current
}
testCorpus = Corpus(VectorSource(l))
testCorpus = tm_map(testCorpus,content_transformer(removeWords),stopwords("en"))
#testWithoutEN = tm_map(testCorpus,content_transformer(removeWords),stopwords("en"))
```

TODO: Cross Validate for TF vs. TF-IDF
```{r}
#construct Document Term Matrix and convert to TF-IDF
DTMTrainSimple = DocumentTermMatrix(trainingCorpus,control = list(weighting = function(x) weightTfIdf(x)))

```

TODO: Drop Frequent Terms
```{r}
dropTermPercent = .95
DTMTrainSimple =removeSparseTerms(DTMTrainSimple,dropTermPercent)
```

Run PCA...
```{r}
pcaTrainSimple = prcomp(DTMTrainSimple,scale = TRUE)
```
...and plot _sdev_ for each number of Principle Components:
```{r}
par(bg = "gray")
plot(pcaTrainSimple$sdev^2,type = "l",main = "Projection Error for number of Principle Components",col = "blue",xlab = "Number of Principal Components",ylab = "Projection Error (Variance)")
bestComponents = 150
abline(v = bestComponents ,lw = 2,col = "red")
```

Next, construct your Document Vectors

```{r}
x = as.matrix(DTMTrainSimple)
V = t(t(pcaTrainSimple$rotation)[1:bestComponents,])
x = x %*% V
```

As a final part of pre-processing, construct your response vector
```{r}
response = sort(rep(allAuthorsInTrainingSet,50))
```

Format your testset:
```{r}
testCorpus = tm_map(testCorpus, content_transformer(tolower)) # make everything lowercase
testCorpus = tm_map(testCorpus, content_transformer(removeNumbers)) # remove numbers
testCorpus = tm_map(testCorpus, content_transformer(removePunctuation)) # remove punctuation
testCorpus = tm_map(testCorpus, content_transformer(stripWhitespace))

```
Turn Test Corpus into DTM matrix:
```{r}
DTMTestSet = DocumentTermMatrix(testCorpus,control = list(weighting = function(x) weightTfIdf(x)))
```

Apply the training set's PCA:
```{r}
pcaTest = predict(pcaTrainSimple,newdata = DTMTestSet)
testX = pcaTest[,1:bestComponents]
```

Create test set response (same as training set response because of directory file structure) and create classification rate function
```{r}
testResponse = response

getClassificationRate = function(model,testX,testY){
  pred = predict(model,data.frame(testX))
  sum(pred == testY)/length(pred)
}
```

Run a Naive Bayes on the data
```{r}
library(naivebayes)
nb = naive_bayes(x = x, y= as.factor(response))
getClassificationRate(nb,testX,testResponse)
```

Decision Tree Bagging
```{r}
library(randomForest)
library(doParallel)
registerDoParallel(cores = 6)
testTrees = c(1,2,5,10,20,40,60,100,120,180,200)
bagClass = foreach( i = 1:length(testTrees),.combine = 'c') %dopar%
{
    bag = randomForest::randomForest(as.factor(response) ~.,data = data.frame(x),mtry =bestComponents,ntree = testTrees[i])
    getClassificationRate(bag,testX,testResponse)  
}

```

Plot the outcome of the Decision Tree
```{r}
par(bg = "gray")
plot(testTrees,randomForestClass,type = "l", col = "red",main = "Bagging in Decision Trees: Classification Rate vs Number of Trees",xlab = "Number of Trees",ylab = "Classification Rate")
print(paste("Best Out-of-Sample Classification Rate: ", max(bagClass)))
```
This is still a very low classification rate. Let's try a Gradient Boosted Tree model

```{r}
library(gbm)
boost = gbm::gbm(as.factor(response)~.,data = data.frame(x),n.trees = 100,shrinkage = .01,interaction.depth = 3)
pred = predict(boost,data.frame(testX),n.trees = 50)
pred = colnames(pred)[apply(data.frame(pred),1,which.max)]
sum(pred == testResponse)/length(testResponse)
```
Boosting works better than Random Forest or Bagging and Naive Bayes. Yet, there is still a relatively low classfication error. This author attribution task was very difficult for these models. However, this model does perform well over the baseline of 2%. 

Using boosting as our model going forward, let's take a look at this model's predictions per author.
```{r}
library(dplyr)
authorClassification = data.frame(pred) %>% group_by(pred) %>% summarise(count = n()) %>% arrange(desc(count))
```


The boosted model was over predicting authors:
```{r}
authorClassification[1:5,]
```

And under predicting authors:
```{r}
authorClassification[(nrow(authorClassification)-5):nrow(authorClassification),]
```